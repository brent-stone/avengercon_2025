{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Avengercon IX Workshop  Python Data Pipelines and Parsers","text":"<p>Repo Status  </p> <p>Tooling  </p> <p> </p> <p>Workshop Date: 26 February 2025</p> <p>The Avenercon IX conference homepage has details about the event where this workshop was held.</p> <p>This is a hands-on Python programming workshop. At least one year of recent Python experience, some experience with Docker, and a computer you administer is strongly recommended.</p> <p>Python's great power is rapid and comprehensible data analysis feeding tools like business intelligence (BI) dashboards and artificial intelligence (AI) models that help organizations make data-driven decisions. This workshop will prepare  you to transition from initial (and likely haphazard) data analysis into understandable, scalable, and automated best  practices for getting data into a place and format that provides value. Pydantic, Celery, and Dask will be covered with  data input and output to a local MinIO S3 bucket.</p>"},{"location":"developers/cloudflare/","title":"Cloudflare","text":""},{"location":"developers/cloudflare/#cors","title":"CORS","text":"<p>Configuring CORS in cloudflare documentation covers several methods, particularly with CORS pre-flight requests, for effectively controlling CORS and preventing issues.</p>"},{"location":"developers/cloudflare/#application-audience-aud-tag","title":"Application Audience (AUD) Tag","text":"<p>Cloudflare Access applications have a dynamically generated AUD tag which will need to be used by any services (e.g. API) to authenticate a Cloudflare Access generated JWT.</p> <p>The online documentation describes where to find the AUD with examples of decoding Cloudflare's JWTs in Go, Python, and Javascript.</p>"},{"location":"developers/cloudflare/#team-domain","title":"Team Domain","text":"<p>The 'team domain' is another unique piece of information used by cloudflare's SDKs to do things like dynamically retrieve the appropriate client PKI keys for a cloudflare zero-trust application. This information can be retrieved from the zero-trust dashboard under <code>Settings &gt; Custom Pages</code>.</p>"},{"location":"developers/docker/","title":"Docker","text":""},{"location":"developers/docker/#testing-dockerfile-setups","title":"Testing <code>Dockerfile</code> setups","text":"<p>To quickly verify whether a Dockerfile is properly configured, try independently running <code>docker build</code> outside of docker compose.</p> <p>For example, with the Api.Dockerfile: <pre><code>docker build -t &lt;image_name&gt; -f &lt;dockerfile_path&gt; .\n</code></pre></p>"},{"location":"developers/docker/#docker-commands","title":"Docker Commands","text":"<ul> <li><code>docker exec -it [CONTAINER_ID] /bin/bash</code> or <code>/bin/sh</code> to get terminal a container</li> <li><code>docker ps</code> lists running containers, their status, and ports</li> <li>Remove 'everything' in Docker (images, etc.): <code>docker system prune -a</code></li> <li>Remove all containers: <code>docker container prune</code></li> <li>Remove all volumes: <code>docker volume prune</code></li> <li>Remove all 'dangling' images: <code>docker rmi $(docker images -f \"dangling=true\" -q)</code></li> <li>List containers: <code>docker container ps --all</code></li> <li>Stop container: <code>docker stop &lt;container-id&gt;</code></li> <li>Delete stopped container: <code>docker rm &lt;container-id&gt;</code></li> <li>List images: <code>docker images</code></li> <li>Delete image: <code>docker image rm &lt;image-id&gt;</code></li> <li>Docker Compose Anchors and Extensions</li> </ul>"},{"location":"developers/docker/#docker-compose","title":"Docker Compose","text":"<p>Docker compose 2 looks for <code>docker-compose.yml</code> to begin orchestration. However, it then also looks at <code>docker-compose.override.yml</code> to override/merge the settings from the base configuration. See the merge compose files documentation for the detailed rules on how this happens.</p>"},{"location":"developers/docs/","title":"Documentation","text":""},{"location":"developers/docs/#mkdocs-material","title":"mkdocs-material","text":"<p>This project uses mkdocs-material to manage and dynamically generate documentation as a set of static websites.</p> <p>To generate and view the compiled documentation locally, run the following within the activated poetry virtual environment:</p> <p><pre><code>mkdocs build\n</code></pre> <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"developers/docs/#admonitions","title":"Admonitions","text":"<p>The admonitions extension is used to provide callouts. The list of available admonitions is available to customize callouts.</p> <p>Examples:</p> <pre><code>!!! note\n    This is a note.\n</code></pre> <p>Note</p> <p>This is a note.</p> <pre><code>!!! warning\n    This is a warning.\n</code></pre> <p>Warning</p> <p>This is a warning.</p>"},{"location":"developers/docs/#buttons-and-icons","title":"Buttons and Icons","text":"<p>The icon search page has a quick lookup of available icons that mkdocs-material supports.</p> <pre><code>[Send :fontawesome-solid-paper-plane:](#){ .md-button }\n</code></pre> <p>Send </p>"},{"location":"developers/docs/#example-sites-using-mkdocs-material","title":"Example Sites Using Mkdocs-Material","text":"<ul> <li>up42 is a nice example of customizing mkdocs-material defaults.</li> </ul>"},{"location":"developers/gitlab_ci/","title":"GitLab CI","text":"<p>Gitlab Continuous Integration (CI) is a way for automatically testing and building artifacts when commits are made to certain branches. For example, base container images, the mkdocs-material documentation hosted on GitLab Pages, etc.</p>"},{"location":"developers/gitlab_ci/#testing-ci-locally","title":"Testing CI locally","text":"<p>The gitlab-runner is available as both an installable binary or docker image.</p> <p>To test a CI pipeline before committing (and thus triggering a 'real' CI job), the gitlab-runner can be used locally.</p> <p>Locally installed binary example run to generate mkdocs documentation site: <pre><code>gitlab-runner exec docker .pages-localhost\n</code></pre></p> <p>Docker-in-docker deployment: <pre><code>docker run gitlab/gitlab-runner exec docker .pages-localhost\n</code></pre></p> <p>Note</p> <p>Additional details of publishing mkdocs-material generated sites to GitLab is available here.</p>"},{"location":"developers/gitlab_ci/#using-ci-to-build-and-pushing-images-to-the-container-registry","title":"Using CI to build and pushing images to the container registry","text":"<p>To use GitLab CI/CD to build and push images, this documentation walks through each step of the process.</p>"},{"location":"developers/gitlab_ci/#manually-publishing-images-to-the-container-registry","title":"Manually publishing images to the container registry","text":"<ul> <li>See this page for documentation.</li> <li>Look for the \"CLI Commands\" button in top right corner of project's Packages &amp; registries menu.</li> <li><code>docker login registry.gitlab.com</code></li> <li>Use a personal access token for your password</li> <li>Use a URL style tag for the image then build it.</li> <li><code>docker build -t registry.gitlab.com/my-group/my-project/&lt;image_name&gt; .</code></li> <li>Use the URL style tag to then push to the container registry.</li> <li><code>docker push registry.gitlab.com/my-group/my-project/&lt;container_name&gt;</code></li> </ul>"},{"location":"developers/os/","title":"Operating Systems","text":""},{"location":"developers/os/#general-os-configurations","title":"General OS Configurations","text":""},{"location":"developers/os/#windows","title":"Windows","text":"<p>Warning</p> <p>Update your git settings so all files are not auto-converted to Windows style line endings: <code>git config --global core.autocrlf false</code></p>"},{"location":"developers/os/#docker-buildkit","title":"Docker Buildkit","text":"<p>Enabling Docker BuildKit will help ensure pip and npm aren't constantly downloading the same packages over and over again.    - On your host machine, enable BuildKit using an environment variable: <code>export DOCKER_BUILDKIT=1</code>    - To permanently set the DOCKER_BUILDKIT flag on Ubuntu:      - <code>gedit ~/.profile</code>      - Add the following to the .profile: <code>export DOCKER_BUILDKIT=1</code></p>"},{"location":"developers/os/#available-scripts","title":"Available Scripts","text":"<p>Be sure to mark scripts as executable with <code>chmod +x &lt;script_path&gt;</code></p> <ul> <li><code>run_dev.sh</code> runs a Docker Compose deployment in a Development configuration.</li> <li><code>soft_reset.sh</code> removes containers and resets <code>.env</code> configs. Volumes and stateful authentication credentials such as passwords are retained.</li> <li><code>full_reset.sh</code> removes containers and resets <code>.env</code> configs. Volumes are deleted.</li> <li><code>initialize_env.sh</code> attempts to create a new <code>.env</code> deployment config.</li> <li><code>export_poetry_to_req_txt.[sh|bat]</code> use poetry to export the various Python environment dependencies to <code>requirements.txt</code> format compatible with <code>pip</code>.</li> <li><code>update_tooling.[sh|bat]</code> attempts to run self-update features of the recommended dev tooling.</li> <li><code>setup_linux_kvm_amd.sh</code> helps set up a Docker user and virtualization on Linux</li> <li><code>force_poetry_shell.sh</code> helps activate a poetry virtual environment in the current terminal when <code>poetry shell</code> isn't working.</li> </ul>"},{"location":"developers/postgres/","title":"PostgreSQL","text":""},{"location":"developers/postgres/#postgresql-container-management","title":"PostgreSQL Container Management","text":""},{"location":"developers/postgres/#troubleshooting-postgres-container-authentication","title":"Troubleshooting Postgres container authentication","text":"<ul> <li>It's necessary to prune both containers and volumes related to postgres to easily reset the initial settings.</li> <li>You'll know you adequately reset the postgres container build to grab new settings if you see <code>CREATE DATABASE</code> and extraordinarily long output generated when starting the container.</li> <li>To see the values of a custom enumerated type in Adminer or postgres command line, run the following sql query: <code>SELECT enum_range(NULL::&lt;custom_type_name&gt;)</code></li> </ul>"},{"location":"developers/sqlalchemy_alembic/","title":"SQLAlchemy & Alembic","text":""},{"location":"developers/sqlalchemy_alembic/#alembic-database-migration-tips","title":"Alembic Database Migration tips","text":"<ul> <li>Auto-generate a database revision</li> <li><code>alembic revision --autogenerate -m \"Added table XYZ\"</code></li> <li>Look in alembic &gt; versions for the revision and validate/update as needed.</li> <li>Implement the revision with <code>alembic upgrade head</code></li> </ul>"},{"location":"developers/sqlalchemy_alembic/#sqlalchemy-mapped-class","title":"SQLAlchemy Mapped Class","text":"<p>The walkthrough of SQLAlchemy 2.0 Declarative Mapping helps quickstart correct design pattern understanding.</p> <ol> <li>Declaring a One-to-Many Relationship with cascading deletion.</li> <li>Declaring a Many-to-Many relationship with unique constraints.</li> <li>Use mixins for recurring column name/types like <code>updated_on</code> datetimes.</li> </ol> <p>Note</p> <p>The mixin example references <code>func.now()</code> which isn't pseudocode. Use <code>from sqlalchemy import func</code>.</p> <ol> <li>Combining dataclass &amp; mixins features</li> </ol>"},{"location":"developers/sqlalchemy_alembic/#sqlalchemy-types","title":"SQLAlchemy Types","text":"<p>The SQLAlchemy Type system supports both generic and backend specific types.</p> <p><code>mapped_column()</code> has implied associations between SQLAlchemy types and basic Python types which can be customized.</p>"},{"location":"developers/testing/","title":"Testing","text":""},{"location":"developers/testing/#pytest-tips","title":"Pytest tips","text":"<p>See the <code>pyproject.toml</code> and <code>tox.ini</code> in the source code for a pytest configuration reference.</p> <ul> <li> <p>Run a specific test:</p> <ul> <li><code>pytest tests/basic_test.py -k \"test_import\" -vv</code></li> <li><code>pytest tests/basic_test.py::test_import</code></li> </ul> </li> <li> <p>Run a test with benchmarking (requires pytest-benchmark)</p> <ul> <li><code>pytest tests/vertical_scale_test.py --benchmark-histogram</code></li> </ul> </li> </ul>"},{"location":"developers/traefik/","title":"Traefik","text":""},{"location":"developers/traefik/#configuring-traefik-securely","title":"Configuring Traefik securely","text":"<p>Mozilla SSL Configuration Generator is a nice tool for seeing good default configuration options.</p>"},{"location":"workshop/10_ORM_DBT/","title":"ORMs and DBT","text":""},{"location":"workshop/10_ORM_DBT/#sql-is-king-long-live-sql","title":"SQL is King. Long live SQL","text":""},{"location":"workshop/10_ORM_DBT/#object-relational-mapper-orm","title":"Object Relational Mapper (ORM)","text":"<p>ORMs, like Python's SQLAlchemy, abstract SQL-based operations into object oriented programming (OOP) APIs. This achieves a couple of things</p> <ol> <li>Devs can now interact with SQL-based data stores without needing to know SQL (mostly)</li> <li>SQL makes it VERY easy to do dumb stuff. ORMs minimize this risk</li> <li>Database migration tools like Alembic are able to intelligently infer the difference between a current database configuration, the new configuration you've described in the ORM, and automatically generate a nearly perfect migration script to \"upgrade\" the database.<ul> <li>Each migration can then be versioned just like software. If something goes wrong, an organization can quickly and safely \"downgrade\" to a previous version with a single    command.</li> </ul> </li> </ol>"},{"location":"workshop/10_ORM_DBT/#data-build-tool-dbt","title":"Data Build Tool (DBT)","text":"<p>DBT is far and away becoming a key pillar of modern data engineering. Combine the abstraction of ORMs with the automation provided by Celery, Dask, etc. for SQL-compliant data stores and that's generally where DBT fits in.</p> <p>It's important to note that DBT by itself is popular but it's really the extensive ecosystem being built on top of or integrating with DBT that is powerful. This is somewhat similar to how Python doesn't necessarily have anything to do with AI... but the ecosystem of tools enabling AI has been built with Python.</p>"},{"location":"workshop/11_beam/","title":"Apache Beam","text":""},{"location":"workshop/11_beam/#what-is-apache-beam","title":"What is Apache Beam","text":"<p>The Apache Beam Overview is a nice introduction to the framework. The very summarized explanation is it allows you to write code once, possibly in the language you already use, then run that code on \"production\" computation servers like Apache Flink, Spark, or Google Cloud Platform.</p> <p>There's four main benefits to using Apache Beam:</p> <ol> <li> <p>The SDK forces you to more-or-less correctly use map-shuffle-reduce programming patterns that's ready to scale to the moon .</p> </li> <li> <p>First class support for streaming data via windowing and other functionality make it one of the few frameworks that could be integrated into your organization's \"data firehose\" to apply AI/ML, heuristics, etc. to intelligently flag or retain valuable information that may otherwise go undetected/missed because the volume of data is too much to save into S3 or elsewhere.</p> </li> <li> <p>If your organization already runs a supported runner, then there's a fairly straightforward developer experience (DX) transitioning code you've written on your local computer to production. This DX is truly streamlined if you've got a dev cluster for your runner available that mimics your prod infrastructure.</p> <p>Linkage with GCP Cloud Dataflow</p> <p>Google is the primary contributor to Apache Beam as it underpins their Google Cloud Platform -- Cloud Dataflow service. Be aware that the transition from dev to prod may have more \"sharp edges\" when using runners that aren't Cloud Dataflow. That said, if you ARE using Cloud Dataflow then this transition is very smooth.</p> </li> <li> <p>While potentially restrictive, the available inputs and outputs for Beam helps you and your organization avoid pitfalls of trying to pair a technology (e.g. traditional SQL database like PostgreSQL) that can't gracefully horizontally scale with code that can.</p> </li> </ol> <p>There's also some significant drawbacks, particularly for Python developers using self- hosted runners like an Apache Spark or Apache Flink cluster.</p> <ol> <li> <p>The execution model for \"real\" Beam pipelines, particularly those not written in Java, isn't as trivial as the SDK may imply. See the execution diagram below.</p> </li> <li> <p>For Python devs, you're forever stuck with months old versions of Numpy, Pandas, and other dependencies as the <code>apache-beam</code> Python SDK has exact versions (ctrl+f \"pandas\") it requires.</p> </li> <li> <p>The pain points of keeping the Python environment used by Dask/Prefect/Celery workers synced with local dev environments are magnified by the complex intertwining of Python, Java, and infrastructure orchestrated by Beam.</p> </li> </ol>"},{"location":"workshop/11_beam/#tour-of-beam","title":"\"Tour of Beam\"","text":"<p>Right click and \"open in new tab\"</p> <p>Tour of Beam </p> <p>Tour of Beam is a fully hosted crash course maintained by the Beam authors and won't be running on our local system. Those that would like to try self-hosting are encouraged to instead look at the \"Beam Playground source code\" which includes a docker compose based deployment. As with the \"Tour of Beam source code\", the \"Beam Playground\" has a fully-hosted interactive browser-based interpreter to try out Apache Beam. \"Tour of Beam\" relies on \"Beam Playground\" and Google Cloud Platform (GCP).</p> <p></p>"},{"location":"workshop/11_beam/#real-use-of-apache-beam","title":"'Real' Use of Apache Beam","text":"<p>Complex Infrastructure</p> <p>While somewhat detailed and quirky, this workshop's code base and the previous sections demonstrate how Dask/Prefect and Celery can be deployed and used by a small team. This isn't really the case with Apache Beam. The \"big data\" Apache ecosystem projects like Flink, Spark, and Kafka are non-trivial to get running, much more effectively tune and maintain. The dataflow diagram below from the Beam authors hint at this significant infrastructure investment.</p>"},{"location":"workshop/11_beam/#example-orchestrating-apache-flink","title":"Example: Orchestrating Apache Flink","text":"<p>Apache Flink allows for parallel streaming processing of data from either files (batch) or true streaming (via message broker like Pub/Sub, Kafka, RabbitMQ, etc.).</p> <p>Two ways for python developers to leverage Flink is either indirectly via Apache Beam or the native apache-flink library. For pros and cons of using Apache Beam, see this and this article by Deepak Nagaraj. The complexity highlighted in these article is aluded to by the SDK Harness Config docs for Apache Beam which shows the need to package, maintain, and troubleshoot custom docker images for each execution environment a beam pipeline may try to execute. See the excerpt of this complex workflow for basic Apache Beam tasks below.</p> <p></p>"},{"location":"workshop/1_hello_workshop/","title":"Hello, Workshop!","text":"<p>This is the beginning of our journey. Our goal is simple but not always easy: can we get our developer environment setup?</p> <p></p>"},{"location":"workshop/1_hello_workshop/#get-ready-system-level-installation","title":"Get Ready: System-level installation","text":"<p>Dev Environment Options</p> Local Python, Docker ServicesDocker Only <p>This is the recommended way to participate in the workshop. It ensures you have gone through each step to prepare your local dev environment to rapidly work with different python versions (via <code>pyenv</code>) and build, test, and publish python wheels using <code>poetry</code>.</p> <p>For those who don't want to make any adjustments to their local dev environment beyond running Docker containers, a Kasm Workspace with Ubuntu 22.04 Jammy Jellyfish and Python 3.13 is provided.</p> <p>For those on a Unix operating system:     <pre><code>chmod +x ./run_dev_desktop.sh\n./run_dev_desktop.sh &lt;flag&gt;\n</code></pre></p> <p>For those on a Windows operating system with Windows Subsystem for Linux (WSL) enabled:     <pre><code>bash run_dev_desktop.sh &lt;flag&gt;\n</code></pre></p> <p>For those on an x86/AMD64 architecture CPU (Intel, AMD, etc.), running <code>run_dev_desktop.sh</code> with the <code>-d</code> flag will pull the AMD64 image from GitHub. For those on an ARM architecture CPU (Mac M1, etc.), use the <code>-m</code> flag to pull an ARM compiled workspace from Docker Hub. If neither options work, use the <code>-l</code> flag to locally build the image. Please note that locally building the image may take several minutes to download and build.</p> <p>The Kasm Workspace is accessible at https://localhost:6901/</p> <p>username: kasm_user  password: avengercon-2025</p> <p></p> <p> After successfully loging in for the first time, stop the deployment with <code>ctrl+c</code> or <code>control-c</code> in the terminal used to run the script. Then uncomment the following lines in the appropriate  <code>docker-compose-dev-container.&lt;architecture&gt;.yaml</code> file to get a live sync of the workshop files inside the Kasm Workspace Ubuntu desktop. If you're using a visual IDE like VSCode or PyCharm, select the commented lines and press <code>ctrl + /</code> or <code>\u2318 + /</code> to quickly uncomment.</p> docker-compose-dev-container.ARCHITECTURE.yaml<pre><code>services:\n  kasm:\n    container_name: kasm-ubuntu-desktop\n    image: brentstone/kasm-ubuntu-desktop:0.2025.1-arm\n    ports:\n      - target: 6901\n        published: 6901\n        mode: host\n    networks:\n      - avengercon-dev-desktop\n    volumes:\n      - type: volume\n        source: kasm-ubuntu-volume\n        target: /home/kasm-user\n        read_only: false\n      # Uncomment after initial build/run of the container to enable real-time synchronization between localhost and\n      # in container workshop files\n#      - type: bind\n#        source: ./\n#        target: /home/kasm-user/Desktop/avengercon_2025/\n#        read_only: false\n    environment:\n      # username is kasm_user\n      VNC_PW: avengercon-2025\n</code></pre> <p>With the updated docker-compose file, re-run <code>run_dev_desktop.sh</code> as before (don't forget any flags) and log back into the Kasm Workspace. You should now see an <code>avengercon_2025</code> folder in the Ubuntu desktop.</p> <p> Last Steps!</p> <ol> <li>In Kasm Workspace, open a terminal and run the <code>kasm-ubuntu-python-final-setup.sh</code> script.</li> <li>Verify <code>python --version</code> and <code>poetry --version</code> works in VSCode terminal</li> <li>Add the Python VSCode extension</li> </ol> <p>You'll need the following software installed to begin:</p> <ol> <li>Docker</li> <li>Python 3.13</li> <li>Git</li> </ol> <p>Optional software</p> Python interpreter managerPoetryGit GUI Client <p>To keep Python from messing up your OS configs, It is recommended to install &amp; manage Python interpreters using a manager like pyenv or hatch. This will allow you to add/remove specific and multiple Python environments on your system. Testing your code against multiple versions of Python is a necessity for professional projects!</p> <p>Poetry is an alternative to <code>pip</code>. Those who prefer <code>pip</code> or other Python environment tools are welcome to use the <code>requirements-dev.txt</code> file in the requirements directory of this repo. Why Poetry? While hatch and other tools are promising, as of this workshop Poetry remains one of the best developer experiences (DX) for setup of python virtual environments and modern pyproject.toml based Python packages.</p> <p>Git GUI Clients simply save time. While it may feel nice to flex that you have every possible permutation of Git commands memorized, 2-second point-and-click to stage dozens of files and side-by-side highlighted diffs are the way to go.</p>"},{"location":"workshop/1_hello_workshop/#get-set-prepare-your-virtual-environment","title":"Get Set: Prepare your virtual environment","text":""},{"location":"workshop/1_hello_workshop/#install-the-avengercon-package-and-dependencies","title":"Install the <code>avengercon</code> package and dependencies","text":"<ol> <li>Establish a virtual environment with <code>poetry env activate</code> or use pip<ul> <li>Look for an additional command Poetry recommends you run. It'll like start with <code>source ...</code></li> </ul> </li> <li>Install <code>avengercon</code> &amp; dependencies with <code>poetry install</code> or use pip</li> <li>Confirm that the local <code>avengercon</code> package is available in your virtual environment</li> </ol> <pre><code>(avengercon-py3.13) $ python -m avengercon\nHello, Workshop!\n</code></pre>"},{"location":"workshop/1_hello_workshop/#configure-your-ide-to-use-the-virtual-environment","title":"Configure your IDE to use the virtual environment","text":"<p>VSCode, PyCharm, and most other popular Integrated Development Environment (IDEs) support intellisense and other productivity boosters when properly configured to use the project's virtual environment.</p> <p>Configure IDE to use the Poetry virtual environment</p>  VSCode PyCharm <ol> <li>Open the VSCode Command Pallet with <code>cmd + shift + p</code> or <code>command + shift + p</code></li> <li><code>Python: Select Interpreter</code></li> <li>Select the Python interpreter located in <code>./.venv/...</code></li> </ol> <ol> <li>PyCharm main menu &gt; <code>Settings</code></li> <li><code>Project: avengercon_2025</code> &gt; <code>Python Interpreter</code></li> <li><code>Add Local Interpreter</code> &gt; <code>Select Existing</code></li> <li>Type <code>Poetry</code> &gt; <code>Poetry env to use</code> &gt; select the Python interpreter located in the <code>./.venv/bin</code> folder</li> </ol>"},{"location":"workshop/1_hello_workshop/#generate-your-env-file","title":"Generate your <code>.env</code> file","text":"<p>Using a terminal from the top level <code>avengercon_2025</code> directory, run the <code>initialize_env.sh</code> script to dynamically generate configuration metadata in a <code>.env</code> file.</p> WindowsUnix (Mac/Linux) <pre><code>bash scripts\\initialize_env.sh\n</code></pre> <pre><code>chmod +x scripts/initialize_env.sh\n./scripts/initialize_env.sh\n</code></pre> <p>You should now see a <code>.env</code>, <code>.localhost.env</code>, and <code>.personal.env</code> files in your <code>avengercon_2025</code> directory that looks something like this: .env<pre><code># Used by run_*.sh scripts &amp; avengercon module to dynamically configure localhost\n# development and testing environment variable coordination.\n# Valid log level values: 'critical', 'error', 'warning', 'info', 'debug'\nLOG_LEVEL=info\n# Traefik settings and labels\nHTTP_PORT=57073\nDOMAIN=localhost\n...\n</code></pre></p>"},{"location":"workshop/1_hello_workshop/#launch","title":"\ud83d\ude80 Launch!","text":"<p>Using a terminal from the top level <code>avengercon_2025</code> directory, launch the <code>docker compose</code> deployment using the <code>run_dev.sh</code> script.</p> WindowsUnix (Mac/Linux) <pre><code>bash run_dev.sh\n</code></pre> <pre><code>chmox +x run_dev.sh\nrun_dev.sh\n</code></pre> <p>Keep your containers deployed</p> <p>Don't close the terminal running your deployment! We'll be using the logs that appear to help monitor the state of our workshop's services and code.</p> <p>Port conflicts</p> <p>If you're already running a Traefik reverse proxy or services that use the ports listed in the <code>.env</code>, you will need to adjust the ports being used for this workshop.</p> <p>If everything is going well, you should not see any \"ERR\" or \"ERROR\" in the logs that appear in your terminal. Happy Logs<pre><code>...\navengercon-minio   | Documentation: https://min.io/docs/minio/linux/index.html\navengercon-minio   | Warning: The standard parity is set to 0. This can lead to data loss.\navengercon-proxy   | 2024-02-15T16:35:39Z INF Traefik version 3.0.0-rc1 built on 2024-02-13T13:41:20Z version=3.0.0-rc1\navengercon-proxy   | 2024-02-15T16:35:39Z INF\navengercon-proxy   | Stats collection is disabled.\navengercon-proxy   | Help us improve Traefik by turning this feature on :)\navengercon-proxy   | More details on: https://doc.traefik.io/traefik/contributing/data-collection/\navengercon-proxy   |\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider aggregator aggregator.ProviderAggregator\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider *docker.Provider\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider *traefik.Provider\navengercon-proxy   | 2024-02-15T16:35:39Z INF Starting provider *acme.ChallengeTLSALPN\navengercon-minio   |\navengercon-minio   |  You are running an older version of MinIO released 1 day before the latest release\navengercon-minio   |  Update: Run `mc admin update ALIAS`\navengercon-minio   |\navengercon-minio   |\navengercon-whoami  | 2024/02/15 16:38:02 172.27.0.3:45642 - - [15/Feb/2024:16:38:02 +0000] \"GET / HTTP/1.1\" - -\n...\n</code></pre></p>"},{"location":"workshop/1_hello_workshop/#verify-you-can-access-your-deployed-services","title":"Verify you can access your deployed services","text":"<p>Ensure you can open a web browser to the following local services. For each button, you probably want to right-click and open in new tab.</p> <p>If anything fails to open, double-check the port in the opened link matches the port specified by <code>HTTP_PORT</code> in your <code>.env</code> configuration. (default is <code>57073</code>; l33t speak approximation for \"Stone\")</p> WhoamiRedisMinIOTraefik <p>Whoami </p> whoami.localhost<pre><code>Name: avengercon_whoami\nHostname: b967780eb9c6\nIP: 127.0.0.1\nIP: ###.###.###.###\nRemoteAddr: ###.###.###.###:42012\nGET / HTTP/1.1\nHost: whoami.localhost:57073\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\nAccept-Encoding: gzip, deflate, br\n...\n</code></pre> <p>Redis </p> <p>The login is <code>default</code> and the password is in your <code>.env</code> as the <code>REDIS_PASSWORD</code> value</p> <p></p> <p>MinIO </p> <p>The login and password is in your <code>.env</code> as the <code>MINIO_ROOT_USER</code> and <code>MINIO_ROOT_PASSWORD</code> values</p> <p></p> <p>Traefik </p> <p></p>"},{"location":"workshop/2_data_engineering_primer/","title":"Data Engineering Primer","text":""},{"location":"workshop/2_data_engineering_primer/#ai-is-so-hot-right-now","title":"AI is so hot right now","text":"<p>Data Engineer</p> <p>Data engineers work in a variety of settings to build systems that collect, manage, and convert raw  data into usable information for data scientists and business analysts to interpret. Their ultimate  goal is to make data accessible so that organizations can use it to evaluate and optimize their  performance. - Coursera</p> <p>A popular military proverb goes \"amateurs talk tactics, professionals talk logistics\". The feverish interest in \"AI\" lends itself to a similar observation: \"amateurs talk AI, professionals talk data engineering\". Like tactics and pitched battles, AI is the flashy end result of months and years of deliberate logistical preparation. This workshop focuses on introducing several key concepts for Python programmers to be excellent data engineers that can deliver value and save costs.</p>"},{"location":"workshop/2_data_engineering_primer/#a-very-summarized-overview-of-modern-data-engineering","title":"A very summarized overview of modern data engineering","text":"<ol> <li>Extract (aka \"Map\"): Open the source data into memory or working swap disc</li> <li>Transform (aka \"Shuffle and Reduce\"): Apply expectations of data schema, \"enrich\" with additional context or tooling (AI), and (potentially) ignore or summarize data to  reduce volume (cost) and maximize value</li> <li>Load: Store the result</li> </ol> <p>ETL vs ELT</p> <p>But wait, what is all this about ETL vs ELT?</p> <p>Extract, Transform, Load ETL fundamentally assumed the organization knows a priori all questions it could possibly want to ask of its data or has significant budget constraints it must meet. The ETL was a requirement in the 90s and early 2000s when computers were running databases and simply couldn't store GBs, TBs, or PBs of data... much more in a cost effective manner.</p> <p>Extract, Load, Transform ELT has gained popularity for two reasons. First, magnetic disc storage cost became neglible and technologically easy to maintain in great quantities. A small business can easily maintain half a petabyte of storage in one 42U rack for the cost of a new economy car. Second, large business realized they were losing agility and spending a tremendous amount of time and manpower trying to enforce data schemas in each business unit. The ELT approach instead shifts to robust data engineering tools and teams that automate data quality checks and transformation pipelines to answer specific questions or use cases that clearly provide value. If a business unit changes their data, the fault is immediately detected. Data engineers are empowered to quickly understand what changed and apply an update to affected pipelines.</p>"},{"location":"workshop/2_data_engineering_primer/#types-of-data","title":"Types of Data","text":"<ol> <li>Unstructured: Requires manual human review or specialized processing tools to extract quantifiable data. Powerpoints, PDFs, images, sound recordings, etc.</li> <li>Semi-Structured: Well defined but highly flexible data structures or file formats. JSON, XML, HTML, Excel Spreadsheet, etc.</li> <li>Structured: Well defined and inflexible data structures with fixed data types. SQL Database, CSV, parquet, Pandas DataFrame, Numpy NDArray, etc.</li> </ol> <p>In general, data engineers have two goals. First, convert unstructured and semi- structured data into structured data according to business rules. Second, seek efficiencies (in both cost and operational agility) by refining structured data into \"high quality\"  subsets or summaries ready for analysts, business intelligence (BI) tools, and AI workflows.</p>"},{"location":"workshop/2_data_engineering_primer/#types-of-big-data-storage-technology","title":"Types of \"big data\" storage technology","text":"<p>Online Analytical Processing (OLAP)</p> <ul> <li>Data Lake: Files on a hard drive. Files could be unstructured, semi-structured, or structured. Unlikely to provide full support for SQL queries over all data stored. Optimized for create operations.</li> <li>Data Warehouse: Structured data files partially or fully in memory (RAM). Provides a SQL interface. Optimized for create and read operations.</li> <li>Data Lakehouse: A data lake that provides a SQL interface. Operationally slower but more cost-effective than a data warehouse. Optimized for create and read operations.</li> </ul> <p>Online Transactional Processing (OLTP) - Database: Transactional data storage optimized for create, read, update, and delete operations. Generally not intended for large amounts of data unless implemented using a \"NoSQL\" technology.</p>"},{"location":"workshop/3_pandas/","title":"Pandas with Extras","text":""},{"location":"workshop/3_pandas/#pandas-probably-does-more-than-you-realize","title":"Pandas probably does more than you realize","text":"<p>Pandas has many optional dependencies that greatly extend what the <code>DataFrame</code> API is capable of. A few examples:</p> <ol> <li><code>excel</code>: Directly read and write <code>.xlsx</code> excel files</li> <li><code>xml</code>: Directly read and write <code>.xml</code> semi-structures files</li> <li><code>postgresql</code>,<code>mysql</code>,etc.: Directly read and write to a database</li> <li><code>parquet</code>: Directly read and write to a common data lakehouse format</li> <li><code>aws</code>: Directly read and write to an Amazon Web Services (AWS) S3 bucket (including MinIO)</li> <li><code>gcp</code>: Read and write to Google Cloud Platform (GCP) Cloud Storage</li> </ol> <p>A caution is using the right tool for the job. Excel, XML, and other semi-structured data formats can be difficult to wrangle into the <code>DataFrame</code> structured, typed, and columnar data structure. Pydantic will often be a much better  intermediate tool that creates self documenting code for how business rules were applied to convert semi-structured JSON or XML data into structured data.</p>"},{"location":"workshop/3_pandas/#plain-ol-pandas-workflow","title":"Plain 'ol Pandas Workflow","text":"<p>We'll download the Rainfall of Iranian Cities datasets into <code>/data/IranRainfall</code> to level set on basic use of pandas <code>read_csv()</code> and <code>read_excel()</code>  functions. The data provides monthly precipitation data for 31 cities in Iran from 1901  to 2022 in Command Separated Value (CSV) and Excel Spreadsheet (XLSX) formats.</p>"},{"location":"workshop/3_pandas/#now-with-an-s3-bucket","title":"Now with an S3 bucket","text":"<p>The provided <code>avengercon</code> Python package provides an authenticated MinIO Client as follows:</p> <pre><code>from avengercon.minio import get_minio_client()\n\nmy_client: Optional[Minio] = get_minio_client()\n</code></pre> <p>The avengercon package also includes a Pydantic model with all your MinIO credentials and endpoint information <pre><code>from avengercon.minio.config import minio_config\nprint(minio_config.endpoint)\nprint(minio_config.access_key)\nprint(minio_config.secret_key.get_secret_value())\n</code></pre></p> <p>Likewise, the \"Hello, Workshop!\" page described how to access our local MinIO server which supports creating buckets and drag-drop data uploads.</p> <p>The MinIO Python SDK documentation provides details and example code for the MinIO Client <code>list_buckets()</code>, <code>list_objects()</code>, and <code>get_object()</code> functions.</p> <p>Finally, the Pandas documentation provides an example of how to directly read and write remote files in an S3 bucket. It points to the S3FS Python package it's using for how to authenticate.</p> <p>Can you successfully read a remote file in S3?</p> <ul> <li> <p>Are you able to successfully adjust the <code>file_uploader.py</code> example on the MinIO Python SDK to read a file using the provided <code>get_minio_client()</code> function?</p> </li> <li> <p>Are you able to use the information in your <code>.env</code> file and the Pandas + S3FS documentation to use a Pandas <code>DataFrame</code>'s <code>read_csv()</code> function to directly read from a MinIO S3 bucket?</p> <ul> <li> <p>Hint #1: The <code>storage_options</code> argument with a <code>\"client_kwargs\"</code> keyed nested dictionary  may be an easy approach for authenticating your credentials.</p> </li> <li> <p>Hint #2: Try a search on Google or ChatGPT for \"Pandas read_csv MinIO\" or \"pandas s3fs read from minio\"</p> </li> </ul> </li> </ul>"},{"location":"workshop/4_pydantic/","title":"Pydantic","text":""},{"location":"workshop/4_pydantic/#pythons-semi-structured-data-parser","title":"Python's Semi-Structured Data Parser","text":"<p>If we're using Python to read semi-structured data like JSON or XML that has more than 1-layer of data, we should be using Pydantic. No exceptions. Anything custom we may write is already implemented in Pydantic using a 100x faster rust-optimized backend and is much more maintainable.</p>"},{"location":"workshop/4_pydantic/#checkout-the-documentation","title":"Checkout the documentation","text":"<p>The Pydantic documentation is very user friendly. Look familiar? The author is the one who inspired me (and by consequence, the Joint dev community) to start using <code>mkdocs-material</code>  for markdown based documentation.</p>"},{"location":"workshop/4_pydantic/#taking-pydantic-for-a-test-drive","title":"Taking Pydantic for a test drive","text":"<p>The Warship Dataset includes 40 years of Heavy-Class warship data from Russia,  Turkey, and the United States. Let's download and save it to <code>/data/Warships</code>. Try to parse it with a Python class we make by inheriting from the Pydantic <code>BaseModel</code>.</p> <p>Challenges</p> <ul> <li> <p>Can you create a <code>BaseModel</code> that successfully parses the JSON?</p> </li> <li> <p>Are your fields using specific types to coerce and validate the data?</p> </li> <li> <p>How can we convert our Pydantic model into a Python dictionary?</p> </li> <li> <p>Are you able to use a loop to iterate over all data in the Pydantic model and write out to a CSV formatted file?</p> </li> </ul>"},{"location":"workshop/5_regex/","title":"Regular Expressions","text":""},{"location":"workshop/5_regex/#when-theres-non-standard-semi-structured-data-formats","title":"When there's non-standard semi-structured data formats","text":"<p>Every now and then, we encounter semi-structured data that isn't in a commonly supported format like JSON or XML. One example is Operational Technology (OT) data such as  Controller Area Network (CAN) data collected via Linux's <code>can-utils</code> library's <code>candump</code> command. The output looks something like this:</p> <pre><code>(1544106679.432583) can0 562#122DF953813CA2\n(1544106679.631330) can0 1F0#4C96281B62ACBC20\n(1544106679.830333) can0 687#C3F26E\n(1544106680.029125) can0 412#06C136\n(1544106680.228086) can0 67E#01C65274D19FAC6D\n(1544106680.427065) can0 4FA#5F6502615110B55A\n(1544106680.625958) can0 429#6FB48932FBCECE26\n</code></pre> <p>There's four distinct pieces of information in each line</p> <ol> <li><code>(1544106679.432583)</code> is the unix epoch timestamp in microseconds</li> <li><code>can0</code> is the network name</li> <li><code>564</code> before the <code>#</code> is the destination ID</li> <li><code>122DF953813CA2</code> after the <code>#</code> is the data for the destination</li> </ol>"},{"location":"workshop/5_regex/#solution-regular-expressions","title":"Solution: Regular Expressions","text":"<p>Pythex.org, regexr.com, regex101.com and similar online tools provide ways to rapidly construct a Regular Expression (regex) to parse arbitrary strings using pre-determined rules.</p> <p>Semi-structured to Structured</p> <ol> <li> <p>Can you write a regex that will match a <code>candump</code> data sample?</p> </li> <li> <p>Can you name each matched part of the data as part of the regex?</p> </li> <li> <p>How can we try this in python?</p> </li> <li> <p>Are you able to convert the output of the regex into a Python dictionary?</p> </li> <li> <p>Are you able to output the dictionary you just created to a CSV file?</p> </li> </ol>"},{"location":"workshop/5_regex/#speeding-up-the-slow-native-python-regex-library","title":"Speeding up the slow native Python regex library","text":"<p>There are two options for somewhat increasing the speed of a regex in Python if you need to parse lots of non-standard semi-structured data.</p> <ol> <li> <p>Google's <code>re2</code>: re2 and it's Python wrapper <code>google-re2</code> provides methods for <code>compiling</code> regular expressions. This will require your dev environment to include C compilers and build tools most python packages don't require.</p> </li> <li> <p>Intel's <code>hyperscan</code>: Hyperscan is a largely stale project from Intel that leveraged some of their CPU level optimizations for regular expressions to achieve top performance for the job. Unfortunately, it's not easily used in Python and only outperforms <code>re2</code> when doing particularly complex or large scale operations.</p> </li> </ol>"},{"location":"workshop/6_python_wheels/","title":"Python Packages and Wheels","text":""},{"location":"workshop/6_python_wheels/#python-wheels","title":"Python Wheels","text":"<p>To make our Python code re-usable, we'll need to properly arrange it into a package. The special <code>.zip</code> format for a Python package is called a wheel with the <code>.whl</code> filename extension.</p> <p>Python wheels can be distributed manually, stored as part of a GitHub or GitLab repository though Continuous Integration (CI) automation, or published to the PyPi repository where Python's built-in package manager, <code>pip</code>, looks when running <code>pip install &lt;wheel name&gt;</code>.</p>"},{"location":"workshop/6_python_wheels/#poetry","title":"Poetry","text":"<p>This workshop uses Poetry to manage the <code>avengercon</code> Python package.</p> <p>Free Wheeling</p> <ol> <li> <p>Can you use Poetry to export the <code>avengercon</code> package to a <code>.whl</code> file?</p> </li> <li> <p>What is the Poetry command to publish a package directly to PyPi so anyone in the world can use it with a <code>pip install</code>?</p> </li> <li> <p>In a new directory, can you create an entirely new package in a new directory using Poetry?</p> </li> <li> <p>Create a <code>__main__.py</code> file in your newly created python package <pre><code>if __name__ == \"__main__\":\n    print(\"Hello, World!\")\n</code></pre></p> <ul> <li>What is special about <code>__main__.py</code> in a Python package?</li> <li>How can we properly execute the function we just added to <code>__main__.py</code>?</li> </ul> </li> </ol>"},{"location":"workshop/6_python_wheels/#anaconda-and-conda-packages","title":"Anaconda and <code>conda</code> packages","text":"<p>You're likely familiar with Jupyter Notebooks. We'll use some in the next section covering Dask. Jupyter relies on the <code>Anaconda</code> Python package manager and ecosystem. Anaconda uses conda packages instead of wheels. If your Python data engineering code is destined to be mostly run in Jupyter Notebooks and an Anaconda environment, you'll instead want to checkout the conda packing documentation</p>"},{"location":"workshop/7_dask/","title":"Dask","text":"<p>Deploy the Dask Services</p> <p>Stop your currently running containers started with <code>run_dev.sh</code> and then execute <code>run_dev_dask.sh</code>. Don't forget <code>chmod +x run_dev_dask.sh</code> on unix.</p>"},{"location":"workshop/7_dask/#what-is-dask","title":"What is Dask?","text":"<p>The Dask Tutorial and this article by NVIDIA has decent infographics and explanations on what Dask is. The VERY summarized explanation is it's a library that combines Tornado and Pandas so that an arbitrary number of Python interpreters and Pandas DataFrames can be used as if they were a single interpreter and DataFrame.</p> <p>The Journey of a Task explanation by the Dask authors provides a nice end-to-end primer on how the framework operates.</p> <p>It is also worth noting that the Ray Python GPU acceleration framework is rapidly growing in popularity. It can be used as a standalone framework or in conjunction with Dask, Celery, Prefect, Airflow, and other  Python data engineering tools.</p>"},{"location":"workshop/7_dask/#what-is-coiled-and-prefect","title":"What is Coiled and Prefect?","text":"<p>Dask fits into a growing segment of the data/tech industry where Free and Open Source Software (FOSS) is provided with fully-managed and extended offerings made available by the primary contributors to make an income.</p> <p>Two of the more prominent companies aligned with Dask are Coiled.io and Prefect. Coiled is basically a fully-managed Dask cluster while Prefect is an expanded offering more geared towards ETL pipelines.</p>"},{"location":"workshop/7_dask/#dask-created-hands-on-crash-course","title":"Dask created hands-on crash course","text":"<p>Dask </p> <p></p> <p>Jupyter Notebook </p> <p>Transition to the official crash-course running on your computer to get comfortable with the framework.</p> <p></p> <p>Your login token will be listed in the terminal next to an <code>avengercon-dask-notebook</code> log entry. You'll need to  copy-paste just the token portion of the url</p> <pre><code>avengercon-dask-notebook       | [... ServerApp] Jupyter Server 2.12.5 is running at:\navengercon-dask-notebook       | [... ServerApp] http://fd52fdf68911:8888/lab?token=b952e22de792f69923d281c04f66393518cd74a0c7fd1acf\n                                            EVERYTHING AFTER THE = IS YOUR TOKEN    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\navengercon-dask-notebook       | [I 2024-02-17 19:42:45.576 ServerApp]     http://127.0.0.1:8888/lab?token=b952e22de792f69923d281c04f66393518cd74a0c7fd1acf\navengercon-dask-notebook       | [I 2024-02-17 19:42:45.576 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre>"},{"location":"workshop/8_celery/","title":"Celery & Flower","text":"<p>Deploy the Celery and Flower Services</p> <p>Stop you currently running containers started with <code>run_dev_dask.sh</code> and then execute <code>run_dev_celery.sh</code>. Don't forget <code>chmod +x run_dev_celery.sh</code> on unix.</p>"},{"location":"workshop/8_celery/#what-is-celery","title":"What is Celery","text":"<p>Celery is a pure-Python implementation of what Dask calls bags or Prefect calls tasks and flows. It is designed to coordinate data among machines and processes using brokers like Redis, RabbitMQ, and Amazon SQS. It can also store the results of code directly to a backend you're probably already using such as AWS S3, Azure Blob, Elasticsearch, various databases, and plain files.</p> <p>It's a relatively simple framework compared to Dask or Apache Beam since it only requires Python to work. Like Dask, it has a scheduler helping work get passed to workers that actually run the code. Like Prefect, it's generally embedded into your own codebase using the <code>@task</code> decorator above your functions.</p>"},{"location":"workshop/8_celery/#what-is-flower","title":"What is Flower","text":"<p>right click and \"open in new tab\"</p> <p>Celery </p> <p></p> <p>Flower is a companion project for Celery to provide a GUI for the scheduler similar to what we saw with Dask and Prefect (albeit not as pretty).</p>"},{"location":"workshop/8_celery/#getting-started-with-celery-the-canvas-api","title":"Getting started with Celery: the Canvas API","text":"<p>Celery calls its main workflow API canvas.</p> <p>The use guide does a good job kick-starting understanding but here is a quick summary of the ideas:</p> <ol> <li> <p>The Celery App is a long-running process that essentially starts the scheduler server. Here we'll call this run time Python object <code>app</code>.</p> </li> <li> <p>Individual Python functions are made into Celery Tasks by registering them with a decorator above the function declaration like <code>@app.task</code></p> </li> <li> <p>Chains are when there's a sequential series of steps you'd like placed together. These steps can fork &amp; recombine, represent a parallel computation (via Groups), an individual Task, etc. Each part of the chain can get all, some, or none of the output created by previous steps.</p> </li> <li> <p>Groups are for when there's multiple Tasks, Chains, other groups, etc. that aren't dependent on each other that you'd like to execute in parallel.</p> </li> </ol> <p>That's it!. There's more to the API but the vast majority of working with Celery is arranging Tasks into a Directed Acyclic Graph (DAG) via Chains and Groups.</p>"},{"location":"workshop/8_celery/#tips-for-working-with-the-celery","title":"Tips for working with the Celery","text":"<ol> <li> <p>Check out the Tips and Best Practices section of the documentation.</p> </li> <li> <p>Celery Tasks can't call Celery Tasks. They can't be \"nested\"</p> </li> <li> <p>Default behavior is for Celery tasks to have preset timeouts It's worth taking time to understand how to adjust timeouts for tasks.</p> </li> <li> <p>When using Chains, if a previous Task produces an output then the following Task will be passed that as in input. This isn't a bad thing but makes it tricky to craft your function declarations without doing some guess-and-check to determine exactly what gets passed between functions.</p> </li> <li> <p>Think of inter-task communication like a webpage calling back to a server. When using the default JSON serializer, it's important to realize that integers like 1 will arrive as the string \"1\". Also, Python objects like a Pandas Dataframe need to be converted to something that can be serialized into a string value in JSON.</p> </li> <li> <p>Unlike Dask or Apache Beam that help minimize the possibility of race conditions, be very careful when using Celery to work with a database or other \"source of truth\" shared by Tasks executing in parallel via a Group.</p> </li> <li> <p>Make sure the Celery Application \"sees\" your tasks. The first step is to add anywhere you've declared tasks with the <code>@app.task</code> decorated to the App's <code>include</code></p> </li> </ol> <p>Including tasks with the Celery App</p> avengercon.celery.tasks.py<pre><code>from avengercon.celery import celery_server\n\n@celery_server.task(name=\"tasks.hello_avengercon\", ignore_result=False)\ndef hello_avengercon() -&gt; str:\n    return \"Hello, AvengerCon! &lt;3 Celery\"\n</code></pre> avengercon.celery.config.py<pre><code>include = [\n    \"avengercon.celery.tasks\",\n]\n</code></pre> celery.py<pre><code>from avengercon.celery import config\nfrom celery import Celery\n\ncelery_server: Celery = Celery(main=\"avengercon\")\ncelery_server.config_from_object(obj=config, silent=False, force=True)\n</code></pre> <p>When starting the Celery App, you'll see the tasks listed in the logs Terminal Logs<pre><code>avengercon-celery              |  -------------- celery@1f7940b050a2 v5.3.6 (emerald-rush)\navengercon-celery              | --- ***** -----\navengercon-celery              | -- ******* ---- Linux-6.5.0-21-generic-x86_64-with-glibc2.36 2024-02-27 15:39:34\navengercon-celery              | - *** --- * ---\navengercon-celery              | - ** ---------- [config]\navengercon-celery              | - ** ---------- .&gt; app:         avengercon:0x7f391a996250\navengercon-celery              | - ** ---------- .&gt; transport:   redis://:**@redis:6379/0\navengercon-celery              | - ** ---------- .&gt; results:     redis://:**@redis:6379/0\navengercon-celery              | - *** --- * --- .&gt; concurrency: 24 (prefork)\navengercon-celery              | -- ******* ---- .&gt; task events: OFF (enable -E to monitor tasks in this worker)\navengercon-celery              | --- ***** -----\navengercon-celery              |  -------------- [queues]\navengercon-celery              |                 .&gt; celery           exchange=celery(direct) key=celery\navengercon-celery              |\navengercon-celery              |\navengercon-celery              | [tasks]\navengercon-celery              |   . tasks.hello_avengercon\navengercon-celery              |\navengercon-celery              | [2024-02-27 15:39:35,400: INFO/MainProcess] Connected to redis://:**@redis:6379/0\navengercon-celery              | [2024-02-27 15:39:35,401: INFO/MainProcess] mingle: searching for neighbors\navengercon-celery              | [2024-02-27 15:39:36,405: INFO/MainProcess] mingle: all alone\n</code></pre></p>"},{"location":"workshop/9_automation/","title":"Automation","text":""},{"location":"workshop/9_automation/#airflow","title":"Airflow","text":"<p>Airflow is the \"OG\" of Python data engineering automation. It's offered as open source code or a fully managed service on various cloud providers (e.g. GCP's DataProc). It still retains a large market share but can be difficult to use. These difficulties led to the rise of newer tools like Prefect or Pachyderm.</p>"},{"location":"workshop/9_automation/#prefect","title":"Prefect","text":"<p>Prefect allows automation and monitoring of Python by adding  decorators (<code>@</code>) above otherwise normal Python functions. The way your code is packaged and automated is very similar the Celery... just much more polished.</p>"},{"location":"workshop/9_automation/#pachyderm","title":"Pachyderm","text":"<p>Pachyderm also enabled python data engineering automation but takes a container-centric approach. Unlike Aiflow or Prefect which are mostly Python native tools, Pachyderm packages each step of a workflow in a container to maximize consistency and portability. This approach allows it to expand beyond Python to also support java, scala, and possibly more as time goes on.</p>"},{"location":"workshop/9_automation/#other-fully-managed-or-semi-proprietary-solutions","title":"Other fully managed or semi-proprietary solutions","text":"<p>Here's a quick rundown of other data engineering pipeline tools that have significant market share. These aren't necessarily specific to python but can help augment Python-based ETL or ELT workflows.</p> <ol> <li> <p>Airbyte: Self-managed and fully managed options. Based on largely point-and-click \"connectors\" for source and sinks of data.</p> </li> <li> <p>Dagster: An connector-focused ETL orchestration tool intended to be a more engineer friendly iteration on the Airflow use case.</p> </li> <li> <p>Fivetran: One of the first offerings in the data engineering space and still widely used.</p> </li> <li> <p>Stitch: Another of the earlier data engineering companies.</p> </li> <li> <p>AWS Glue: Amazon Web Service's proprietary ETL pipeline tool.</p> </li> <li> <p>Azure Fabric: Microsoft's fully-managed offering.</p> </li> <li> <p>DBT: Meteorically taking over the ELT market for data pipelines. More on DBT and Object Relational Mapper  (ORM) tooling on the next page.</p> </li> <li> <p>Apache DataFusion: A project gaining in popularity intended to combine DataFrame (internal type, Pandas, or Polars) and SQL based data sources and sinks into an integrated workflow. Integrates with Python, Ray, and Spark.</p> </li> </ol>"}]}