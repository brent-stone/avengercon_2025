# Avengercon IX Workshop <br> Python Data Engineering Automation

Repo Status </br>
![Tests](docs/badges/tests.svg)
![Coverage](docs/badges/coverage.svg)
![Interrogate](docs/badges/interrogate_badge.svg)
<!---
Update the CI status to your repo's project name!
https://docs.github.com/en/actions/monitoring-and-troubleshooting-workflows/adding-a-workflow-status-badge
https://docs.gitlab.com/ee/user/project/badges.html#view-the-url-of-pipeline-badges
-->

Tooling </br>
![Python](docs/badges/python313.svg)
[![Poetry](docs/badges/poetry.svg)](https://python-poetry.org/)
[![Tox](docs/badges/tox.svg)](https://tox.wiki/)
[![Black](docs/badges/black.svg)](https://black.readthedocs.io/en/stable/)
[![Safety](docs/badges/safety.svg)](https://github.com/pyupio/safety)
[![Bandit](docs/badges/bandit.svg)](https://github.com/PyCQA/bandit)

[![Celery](docs/badges/celery.svg)](https://docs.celeryq.dev/en/stable/getting-started/introduction.html)
[![Dask](docs/badges/dask.svg)](https://www.dask.org/)
[![Prefect](docs/badges/prefect.svg)](https://www.prefect.io//)
[![MinIO](docs/badges/minio.svg)](https://min.io/)
[![mkdocs-material](docs/badges/mkdocs-material.svg)](https://squidfunk.github.io/mkdocs-material/)

See [the documentation](https://brent-stone.github.io/avengercon_2025/) for information
on using this repo. The [Avengercon conference](https://avengercon.com/)
homepage has details about the event.

---
This is a hands-on Python programming workshop. At least one year of recent Python
experience, some experience with Docker, and a computer you administer is strongly
recommended.

Python's great power is rapid and comprehensible data analysis feeding tools like 
business intelligence (BI) dashboards and artificial intelligence (AI) models that help 
organizations make data-driven decisions. This workshop will prepare you to transition 
from initial (and likely haphazard) data analysis into understandable, scalable, and 
automated best practices for getting data into a place and format that provides value. 
Pydantic, Celery, and Dask will be covered with data input and output to a local MinIO 
S3 bucket.
